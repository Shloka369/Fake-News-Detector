{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data, Adding column names to it, and merging it as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 16 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(df3\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjustification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m df2\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjustification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m df3\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjustification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Avani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Avani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\Avani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\Users\\Avani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 16 elements"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('train_data.tsv', sep='\\t')\n",
    "df2 = pd.read_csv('test_data.tsv', sep='\\t')\n",
    "df3 = pd.read_csv('Validation_data.tsv', sep='\\t')\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "df1.columns = ['index', 'id', 'label', 'statement', 'subject', 'speaker', 'JobTitle', 'State', 'Party', 'BTC', 'FC', 'HT', 'MT', 'POF', 'context', 'justification']\n",
    "df2.columns = ['index', 'id', 'label', 'statement', 'subject', 'speaker', 'JobTitle', 'State', 'Party', 'BTC', 'FC', 'HT', 'MT', 'POF', 'context', 'justification']\n",
    "df3.columns = ['index', 'id', 'label', 'statement', 'subject', 'speaker', 'JobTitle', 'State', 'Party', 'BTC', 'FC', 'HT', 'MT', 'POF', 'context', 'justification']\n",
    "df = pd.concat([df1, df2, df3], axis=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_copy=df.copy()\n",
    "df_copy.drop(['statement','subject','justification'],axis=1,inplace=True)\n",
    "df_copy['id'] = df_copy['id'].apply(lambda x: x[:-5])\n",
    "df_copy['id'] = df_copy['id'].astype('int64')\n",
    "le = LabelEncoder()\n",
    "df_copy['label'] = le.fit_transform(df_copy['label'])\n",
    "df_copy['speaker'] = le.fit_transform(df_copy['speaker'])\n",
    "df_copy['JobTitle'] = le.fit_transform(df_copy['JobTitle'])\n",
    "df_copy['State'] = le.fit_transform(df_copy['State'])\n",
    "df_copy['Party'] = le.fit_transform(df_copy['Party'])\n",
    "print(df_copy.head())\n",
    "corr_matrix = df_copy.corr()\n",
    "plt.figure(figsize=(11,11))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BTC',y='FC',data=df_copy)\n",
    "plt.show()\n",
    "sns.scatterplot(x='POF',y='HT',data=df_copy)\n",
    "plt.show()\n",
    "sns.scatterplot(x='MT',y='FC',data=df_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping non-required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['index', 'id', 'JobTitle', 'State', 'BTC', 'FC', 'HT', 'MT', 'POF', 'context', 'justification'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data into binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map({'true': 1, 'half-true': 1, 'mostly-true': 1, 'false': 0, 'pants-fire': 0, 'barely-true': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the statement and subject columns into one column for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['subject'] + ' ' + df['statement']\n",
    "df = df.drop(['subject', 'statement'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping missing values rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"We drop the missing values\")\n",
    "df = df.dropna()\n",
    "print(\"The shape of the dataset is now: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations except comma, any links and any extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('[^\\w\\s,]', '')\n",
    "df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "df['text'] = df['text'].str.replace(' ,', ',')\n",
    "df['text'] = df['text'].str.replace(', ', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['text'] = df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization of text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words removal from text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining text column into a string for vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "all_words = ' '.join([text for text in df['text']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TFIDF and BOW for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector_Tfidf(df, col):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=2000)\n",
    "    vectorizer.fit(df[col])\n",
    "    return vectorizer.transform(df[col])\n",
    "\n",
    "def to_vector_bow(df, col):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(max_features=2000)\n",
    "    vectorizer.fit(df[col])\n",
    "    return vectorizer.transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_tfidf = to_vector_Tfidf(df, 'text')\n",
    "print(\"Shape of the tfidf vector: \", text_vector_tfidf.shape)\n",
    "print(text_vector_tfidf.shape)\n",
    "text_vector_bow = to_vector_bow(df, 'text')\n",
    "print(\"Shape of the text vector for bow vectorization: \", text_vector_bow.shape)\n",
    "print(text_vector_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_tfidf_copy=text_vector_tfidf.copy()\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(text_vector_tfidf_copy.toarray())\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca.components_[0], pca.components_[1])\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA on tfidf vector')\n",
    "plt.show()\n",
    "\n",
    "text_vector_bow_copy=text_vector_bow.copy()\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(text_vector_bow_copy.toarray())\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca.components_[0], pca.components_[1])\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA on bow vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_text_vector_tfidf_tsne_copy=text_vector_tfidf.copy()\n",
    "tsne_text_vector_tfidf_tsne_copy = tsne.fit_transform(tsne_text_vector_tfidf_tsne_copy.toarray())\n",
    "plt.figure(figsize=(10, 7))\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=tsne_text_vector_tfidf_tsne_copy[:,0], y=tsne_text_vector_tfidf_tsne_copy[:,1], hue=df['label'])\n",
    "plt.title('TSNE on tfidf vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_text_vector_bow_tsne_copy=text_vector_bow.copy()\n",
    "tsne_text_vector_bow_tsne_copy = tsne.fit_transform(tsne_text_vector_bow_tsne_copy.toarray())\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=tsne_text_vector_bow_tsne_copy[:,0], y=tsne_text_vector_bow_tsne_copy[:,1], hue=df['label'])\n",
    "plt.title('TSNE on bow vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_tfidf = text_vector_tfidf.toarray()\n",
    "text_vector_bow = text_vector_bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vector = df['label'].values\n",
    "speaker_vector = df['speaker'].values\n",
    "party_vector = df['Party'].values\n",
    "label_vector = label_vector.reshape(-1, 1)\n",
    "speaker_vector = speaker_vector.reshape(-1, 1)\n",
    "party_vector = party_vector.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF1 = np.concatenate((text_vector_tfidf, label_vector), axis=1)\n",
    "dataF2 = np.concatenate((text_vector_bow, label_vector), axis=1)\n",
    "dataF3 = np.concatenate((text_vector_tfidf, label_vector, speaker_vector, party_vector), axis=1)\n",
    "dataF4 = np.concatenate((text_vector_bow, label_vector, speaker_vector, party_vector), axis=1)\n",
    "print(dataF1.shape)\n",
    "print(dataF2.shape)\n",
    "print(dataF3.shape)\n",
    "print(dataF4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF1=pd.DataFrame(dataF1)\n",
    "dataF2=pd.DataFrame(dataF2)\n",
    "dataF3=pd.DataFrame(dataF3)\n",
    "dataF4=pd.DataFrame(dataF4)\n",
    "print(dataF1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    dataF1.rename(columns={i: 'tfidf'+str(i)}, inplace=True)\n",
    "    dataF2.rename(columns={i: 'bow'+str(i)}, inplace=True)\n",
    "    dataF3.rename(columns={i: 'tfidf'+str(i)}, inplace=True)\n",
    "    dataF4.rename(columns={i: 'bow'+str(i)}, inplace=True)\n",
    "dataF1.rename(columns={2000: 'label'}, inplace=True)\n",
    "dataF2.rename(columns={2000: 'label'}, inplace=True)\n",
    "dataF3.rename(columns={2000: 'label'}, inplace=True)\n",
    "dataF4.rename(columns={2000: 'label'}, inplace=True)\n",
    "dataF3.rename(columns={2001: 'speaker'}, inplace=True)\n",
    "dataF4.rename(columns={2001: 'speaker'}, inplace=True)\n",
    "dataF3.rename(columns={2002: 'party'}, inplace=True)\n",
    "dataF4.rename(columns={2002: 'party'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le3=LabelEncoder()\n",
    "le4=LabelEncoder()\n",
    "dataF3['speaker']=le3.fit_transform(dataF3['speaker'])\n",
    "dataF4['speaker']=le4.fit_transform(dataF4['speaker'])\n",
    "dataF3['party']=le3.fit_transform(dataF3['party'])\n",
    "dataF4['party']=le4.fit_transform(dataF4['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(dataF1.drop('label', axis=1), dataF1['label'], test_size=0.2, random_state=0)\n",
    "X_val1, X_test1, y_val1, y_test1 = train_test_split(X_test1, y_test1, test_size=0.5, random_state=0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(dataF2.drop('label', axis=1), dataF2['label'], test_size=0.2, random_state=0)\n",
    "X_val2, X_test2, y_val2, y_test2 = train_test_split(X_test2, y_test2, test_size=0.5, random_state=0)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(dataF3.drop('label', axis=1), dataF3['label'], test_size=0.2, random_state=0)\n",
    "X_val3, X_test3, y_val3, y_test3 = train_test_split(X_test3, y_test3, test_size=0.5, random_state=0)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(dataF4.drop('label', axis=1), dataF4['label'], test_size=0.2, random_state=0)\n",
    "X_val4, X_test4, y_val4, y_test4 = train_test_split(X_test4, y_test4, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataF1.head())\n",
    "print(dataF3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search and learning curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    gnb=GaussianNB()\n",
    "    param_grid = {'var_smoothing': np.logspace(0,-9, num=10)}\n",
    "    grid_search = GridSearchCV(gnb, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for Gaussian Naive Bayes\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def logistic_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    logreg=LogisticRegression(random_state=0,max_iter=20000)\n",
    "    param_grid = {'C': [0.1, 1, 10],'tol': [0.0001, 0.001, 1, 10]}\n",
    "    grid_search = GridSearchCV(logreg, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for Logistic Regression\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def decision_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    dtree=DecisionTreeClassifier(random_state=0)\n",
    "    param_grid = {'criterion':[\"gini\",\"entropy\"],'max_depth': [10, 50, 100, None]}\n",
    "    grid_search = GridSearchCV(dtree, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for Decision Tree\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def random_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    rtree=RandomForestClassifier(random_state=0)\n",
    "    param_grid = {'max_depth': [10,50, 100, None],'criterion': ['gini', 'entropy']}\n",
    "    grid_search = GridSearchCV(rtree, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for Random Forest\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def ada_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    ada=AdaBoostClassifier(random_state=0)\n",
    "    param_grid = {'n_estimators': [10, 50],'learning_rate': [0.1,0.5,1,2]}\n",
    "    grid_search = GridSearchCV(ada, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for AdaBoost\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def svm_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    svm=SVC(random_state=0)\n",
    "    param_grid={'C':[0.1,1,100],'kernel':['rbf','linear']}\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1, verbose=1,shuffle=True)\n",
    "    train_mean=np.mean(train_scores,axis=1)\n",
    "    train_std=np.std(train_scores,axis=1)\n",
    "    test_mean=np.mean(test_scores,axis=1)\n",
    "    test_std=np.std(test_scores,axis=1)\n",
    "    plt.plot(train_sizes,train_mean,color='blue',marker='o',label='training accuracy')\n",
    "    plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\n",
    "    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',label='validation accuracy')\n",
    "    plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')\n",
    "    plt.title(\"Learning Curve for SVM\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def mlp_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    mlp=MLPClassifier(random_state=0,early_stopping=True,validation_fraction=0.2)\n",
    "    param_grid={'activation':[\"relu\",\"logistic\"]}\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val,grid_search.predict(X_val)))\n",
    "    print(classification_report(y_val,grid_search.predict(X_val)))\n",
    "    plt.plot(grid_search.best_estimator_.loss_curve_)\n",
    "    plt.plot(grid_search.best_estimator_.validation_scores_)\n",
    "    plt.title(\"Loss Curve and Validation Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Loss Curve\",\"Validation Score\"])\n",
    "    plt.show()\n",
    "\n",
    "def mlp_pca_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    X_train_copy=X_train.copy()\n",
    "    X_val_copy=X_val.copy()\n",
    "    y_train_copy=y_train.copy()\n",
    "    y_val_copy=y_val.copy()\n",
    "    pca=PCA()\n",
    "    X_train_copy=pca.fit_transform(X_train_copy)\n",
    "    X_val_copy=pca.transform(X_val_copy)\n",
    "    mlp=MLPClassifier(random_state=0,early_stopping=True,validation_fraction=0.2)\n",
    "    param_grid={'activation':[\"relu\",\"logistic\"]}\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train_copy,y_train_copy)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val_copy,grid_search.predict(X_val_copy)))\n",
    "    print(classification_report(y_val_copy,grid_search.predict(X_val_copy)))\n",
    "    plt.plot(grid_search.best_estimator_.loss_curve_)\n",
    "    plt.plot(grid_search.best_estimator_.validation_scores_)\n",
    "    plt.title(\"Loss Curve and Validation Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Loss Curve\",\"Validation Score\"])\n",
    "    plt.show()\n",
    "\n",
    "def mlp_tsne_grid_learn(X_train,y_train,X_val,y_val):\n",
    "    X_train_copy=X_train.copy()\n",
    "    X_val_copy=X_val.copy()\n",
    "    y_train_copy=y_train.copy()\n",
    "    y_val_copy=y_val.copy()\n",
    "    tsne=TSNE(n_components=2)\n",
    "    X=np.vstack((X_train_copy,X_val_copy))\n",
    "    X=tsne.fit_transform(X)\n",
    "    X_train_copy=X[:len(X_train_copy)]\n",
    "    X_val_copy=X[len(X_train_copy):]\n",
    "    mlp=MLPClassifier(random_state=0,early_stopping=True,validation_fraction=0.2)\n",
    "    param_grid={'activation':[\"relu\",\"logistic\"]}\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=3,refit=True,n_jobs=-1,scoring='accuracy',verbose=1)\n",
    "    grid_search.fit(X_train_copy,y_train_copy)\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"Accuracy: \",accuracy_score(y_val_copy,grid_search.predict(X_val_copy)))\n",
    "    print(classification_report(y_val_copy,grid_search.predict(X_val_copy)))\n",
    "    plt.plot(grid_search.best_estimator_.loss_curve_)\n",
    "    plt.plot(grid_search.best_estimator_.validation_scores_)\n",
    "    plt.title(\"Loss Curve and Validation Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Loss Curve\",\"Validation Score\"])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pca_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pca_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pca_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pca_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tsne_grid_learn(X_train1,y_train1,X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tsne_grid_learn(X_train2,y_train2,X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tsne_grid_learn(X_train3,y_train3,X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tsne_grid_learn(X_train4,y_train4,X_val4,y_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2075c97d54907e4ce0bb4b751e65d444874ef25bbe90d9b8c2daef915656c098"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
